{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import shutil\n",
    "import random\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.datasets import ImageNet\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "#import torchvision.models as models\n",
    "#from torchvision.transforms import Compose\n",
    "#from torchvision.transforms import ToTensor\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resize(object):\n",
    "    def __init__(self, size, interpolation=Image.BILINEAR):\n",
    "        self.size = size\n",
    "        self.interpolation = interpolation\n",
    "        self.aspect_ratio = []\n",
    "\n",
    "    def __call__(self, img):\n",
    "        old_size = img.size  # old_size[0] is in (width, height) format\n",
    "\n",
    "        ratio = float(self.size)/max(old_size)\n",
    "        self.aspect_ratio.append(ratio)\n",
    "        new_size = tuple([int(x * ratio) for x in old_size])\n",
    "        \n",
    "        img=img.resize(new_size, resample=self.interpolation)\n",
    "        new_im = Image.new(\"RGB\", (self.size, self.size))\n",
    "        new_im.paste(img, ((self.size-new_size[0])//2,(self.size-new_size[1])//2))\n",
    "        return new_im\n",
    "#resize = Resize(224)\n",
    "#img = resize(Image.open('train_all_test/00010.jpg'))\n",
    "#img.save('train_all_test/00010.jpg') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2  = open(\"fname_string.txt\", \"r\").read().split(\"\\n\")\n",
    "\n",
    "resize = Resize(224)\n",
    "for fname in f2:\n",
    "    img = resize(Image.open('train_all/{}'.format(fname)))\n",
    "    img.save('train_all/{}'.format(fname)) \n",
    "AR=resize.aspect_ratio\n",
    "with open('aspectratio.txt', 'w') as f:\n",
    "    for item in AR:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all_path = Path.cwd() / \"train_all\"\n",
    "\n",
    "all_filenames = list(train_all_path.glob(\"*.jpg\"))\n",
    "all_filenames_length = len(all_filenames)\n",
    "#print(all_filenames)\n",
    "\n",
    "train_filenames = random.sample(all_filenames, int(0.8*all_filenames_length))\n",
    "#print(train_filenames)\n",
    "val_filenames=[]\n",
    "for file in all_filenames:\n",
    "    if file not in train_filenames:\n",
    "        val_filenames.append(file)\n",
    "        \n",
    "        \n",
    "for subdirectory in range(1,197):\n",
    "    subdirectory = Path(\"train/{}\".format(subdirectory))\n",
    "    subdirectory.mkdir(parents=True, exist_ok=True)\n",
    "for subdirectory in range(1,197):\n",
    "    subdirectory = Path(\"val/{}\".format(subdirectory))\n",
    "    subdirectory.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "f1  = open(\"class_string.txt\", \"r\").read().split(\"\\n\")\n",
    "     \n",
    "for index in range(all_filenames_length):\n",
    "    rand = np.random.rand()\n",
    "    if rand < 0.8:\n",
    "        shutil.copyfile(all_filenames[index], Path.cwd() / \"train/{}/{}\".format(f1[index],f2[index]))\n",
    "    else:\n",
    "        shutil.copyfile(all_filenames[index], Path.cwd() / \"val/{}/{}\".format(f1[index],f2[index]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00001.jpg', 14, 43, 212, 140]\n"
     ]
    }
   ],
   "source": [
    "f3 = open(\"bbox.txt\", \"r\").read().split(\"\\n\")\n",
    "f1  = open(\"class_string.txt\", \"r\").read().split(\"\\n\")\n",
    "f4  = open(\"aspectratio.txt\", \"r\").read().split(\"\\n\")\n",
    "AR=[]\n",
    "for line in f4:\n",
    "    try:\n",
    "        AR.append(float(line))\n",
    "    except:\n",
    "        pass\n",
    "totbbox = []\n",
    "for i,line in enumerate(f3):\n",
    "    bbox = line.split(\"\\t\") # x1,y1,x2,y2\n",
    "    bbox[0] = int(int(bbox[0])*AR[i])\n",
    "    bbox[1] = int(int(bbox[1])*AR[i])\n",
    "    bbox[2] = int(int(bbox[2])*AR[i])\n",
    "    bbox[3] = int(int(bbox[3])*AR[i])\n",
    "    totbbox.append(bbox)\n",
    "    f1[i] = int(f1[i])\n",
    "#totbbox = torch.tensor(totbbox,dtype=torch.float32)\n",
    "\n",
    "fin_list = []\n",
    "for i in range(len(f2)):\n",
    "    fin_list.append([f2[i],totbbox[i][0],totbbox[i][1],totbbox[i][2],totbbox[i][3]])\n",
    "print(fin_list[0][:])\n",
    "df = pd.DataFrame(fin_list, columns=['image_id', 'x0','y0','x1','y1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Handler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_transforms(transformations, index):\n",
    "    \"\"\"Visually compare transformations side by side.\n",
    "    Takes a list of ImageFolder datasets with different compositions of transformations.\n",
    "    It then display the `index`th image of the dataset for each transformed dataset in the list.\n",
    "    \n",
    "    Example usage:\n",
    "        compare_transforms([dataset_with_transform_1, dataset_with_transform_2], 0)\n",
    "    \n",
    "    Args:\n",
    "        transformations (list(ImageFolder)): list of ImageFolder instances with different transformations\n",
    "        index (int): Index of the sample in the ImageFolder you wish to compare.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Here we combine two neat functions from basic python to validate the input to the function:\n",
    "    # - `all` takes an iterable (something we can loop over, like a list) of booleans\n",
    "    #    and returns True if every element is True, otherwise it returns False.\n",
    "    # - `isinstance` checks whether a variable is an instance of a particular type (class)\n",
    "    if not all(isinstance(transf, ImageFolder) for transf in transformations):\n",
    "        raise TypeError(\"All elements in the `transformations` list need to be of type ImageFolder\")\n",
    "        \n",
    "    num_transformations = len(transformations)\n",
    "    fig, axes = plt.subplots(1, num_transformations)\n",
    "    \n",
    "    # This is just a hack to make sure that `axes` is a list of the same length as `transformations`.\n",
    "    # If we only have one element in the list, `plt.subplots` will not create a list of a single axis\n",
    "    # but rather just an axis without a list.\n",
    "    if num_transformations == 1:\n",
    "        axes = [axes]\n",
    "        \n",
    "    for counter, (axis, transf) in enumerate(zip(axes, transformations)):\n",
    "        axis.set_title(\"transf: {}\".format(counter))\n",
    "        image_tensor = transf[index][0]\n",
    "        display_image(axis, image_tensor)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def display_image(axis, image_tensor):\n",
    "    \"\"\"Display a tensor as image\n",
    "    \n",
    "    Example usage:\n",
    "        _, axis = plt.subplots()\n",
    "        some_random_index = 453\n",
    "        image_tensor, _ = train_dataset[some_random_index]\n",
    "        display_image(axis, image_tensor)\n",
    "    \n",
    "    Args:\n",
    "        axis (pyplot axis)\n",
    "        image_tensor (torch.Tensor): tensor with shape (num_channels=3, width, heigth)\n",
    "    \"\"\"\n",
    "    \n",
    "    # See hint above\n",
    "    if not isinstance(image_tensor, torch.Tensor):\n",
    "        raise TypeError(\"The `display_image` function expects a `torch.Tensor` \" +\n",
    "                        \"use the `ToTensor` transformation to convert the images to tensors.\")\n",
    "        \n",
    "    # The imshow commands expects a `numpy array` with shape (3, width, height)\n",
    "    # We rearrange the dimensions with `permute` and then convert it to `numpy`\n",
    "    image_data = image_tensor.permute(1, 2, 0).numpy()\n",
    "    height, width, _ = image_data.shape\n",
    "    axis.imshow(image_data)\n",
    "    axis.set_xlim(0, width)\n",
    "    # By convention when working with images, the origin is at the top left corner.\n",
    "    # Therefore, we switch the order of the y limits.\n",
    "    axis.set_ylim(height, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[0.4422, 0.5564, 0.1337,  ..., 0.3464, 0.0750, 0.2763],\n",
      "         [0.3008, 0.9526, 0.8631,  ..., 0.6626, 0.9081, 0.0346],\n",
      "         [0.0091, 0.1780, 0.3625,  ..., 0.2065, 0.6453, 0.0682],\n",
      "         ...,\n",
      "         [0.2500, 0.4008, 0.1139,  ..., 0.8823, 0.5413, 0.2733],\n",
      "         [0.1238, 0.7298, 0.7411,  ..., 0.2676, 0.9307, 0.2228],\n",
      "         [0.3523, 0.1417, 0.7861,  ..., 0.7813, 0.6794, 0.9571]],\n",
      "\n",
      "        [[0.6697, 0.9908, 0.6568,  ..., 0.3863, 0.8997, 0.5018],\n",
      "         [0.5269, 0.2296, 0.1138,  ..., 0.1567, 0.5222, 0.3227],\n",
      "         [0.9658, 0.8961, 0.1739,  ..., 0.4766, 0.1538, 0.9061],\n",
      "         ...,\n",
      "         [0.4800, 0.8119, 0.6043,  ..., 0.0031, 0.9129, 0.0232],\n",
      "         [0.0731, 0.1103, 0.6099,  ..., 0.8948, 0.5263, 0.5219],\n",
      "         [0.6220, 0.8854, 0.8412,  ..., 0.8091, 0.6535, 0.8053]],\n",
      "\n",
      "        [[0.7213, 0.2984, 0.3505,  ..., 0.3394, 0.6020, 0.5860],\n",
      "         [0.7458, 0.9605, 0.6568,  ..., 0.5303, 0.7131, 0.5549],\n",
      "         [0.5483, 0.7315, 0.2568,  ..., 0.9959, 0.1906, 0.0501],\n",
      "         ...,\n",
      "         [0.6194, 0.2778, 0.3323,  ..., 0.8638, 0.4724, 0.2823],\n",
      "         [0.4389, 0.5268, 0.9146,  ..., 0.4845, 0.3716, 0.2785],\n",
      "         [0.3749, 0.9161, 0.4993,  ..., 0.9706, 0.1589, 0.3405]]]), tensor([[[6.9488e-02, 8.0589e-01, 5.7004e-01,  ..., 9.1396e-01,\n",
      "          6.1873e-01, 9.1381e-01],\n",
      "         [4.0514e-02, 2.2087e-03, 9.7632e-01,  ..., 6.9999e-01,\n",
      "          3.1574e-01, 1.9552e-01],\n",
      "         [6.0152e-01, 8.5415e-01, 1.9051e-01,  ..., 9.8869e-01,\n",
      "          1.0242e-01, 9.5879e-01],\n",
      "         ...,\n",
      "         [4.6483e-01, 8.4716e-01, 8.1088e-01,  ..., 9.7145e-01,\n",
      "          5.0032e-01, 5.5927e-01],\n",
      "         [7.7692e-01, 9.3982e-01, 4.7814e-01,  ..., 3.0178e-01,\n",
      "          9.7140e-01, 7.4105e-01],\n",
      "         [4.6287e-01, 6.0360e-01, 7.6062e-01,  ..., 9.2178e-01,\n",
      "          6.0749e-01, 2.4015e-01]],\n",
      "\n",
      "        [[7.2290e-01, 6.1264e-01, 8.7204e-01,  ..., 9.6416e-01,\n",
      "          4.4351e-01, 9.6519e-01],\n",
      "         [3.7195e-01, 3.4536e-01, 7.9351e-02,  ..., 9.2408e-01,\n",
      "          2.1490e-01, 1.1426e-04],\n",
      "         [4.6289e-01, 8.5047e-01, 6.6559e-01,  ..., 3.2945e-01,\n",
      "          7.2248e-01, 2.5725e-01],\n",
      "         ...,\n",
      "         [5.2869e-01, 2.0834e-02, 9.1255e-01,  ..., 5.6828e-01,\n",
      "          7.7903e-01, 5.8957e-01],\n",
      "         [1.2601e-01, 6.4916e-01, 9.6923e-01,  ..., 7.7806e-01,\n",
      "          4.5405e-01, 3.1850e-01],\n",
      "         [6.5063e-01, 6.5900e-01, 2.0222e-01,  ..., 5.5147e-02,\n",
      "          1.4637e-02, 9.6748e-01]],\n",
      "\n",
      "        [[3.8347e-01, 5.3175e-02, 7.2700e-01,  ..., 2.4601e-01,\n",
      "          5.6406e-01, 5.6808e-01],\n",
      "         [4.0708e-01, 2.2228e-01, 1.7583e-01,  ..., 6.1101e-01,\n",
      "          8.0882e-01, 3.5527e-01],\n",
      "         [8.5526e-01, 7.4937e-01, 3.1657e-01,  ..., 6.0697e-01,\n",
      "          2.6868e-01, 5.5953e-01],\n",
      "         ...,\n",
      "         [3.1020e-01, 9.0364e-01, 4.3163e-01,  ..., 2.9373e-01,\n",
      "          8.2884e-01, 8.2173e-01],\n",
      "         [3.1680e-01, 2.1885e-01, 7.0997e-01,  ..., 7.2476e-01,\n",
      "          1.5906e-01, 7.0946e-01],\n",
      "         [5.8418e-01, 4.0058e-01, 9.3348e-01,  ..., 7.5656e-01,\n",
      "          7.6144e-01, 2.4393e-01]]]), tensor([[[0.8393, 0.4286, 0.9908,  ..., 0.5379, 0.6892, 0.2509],\n",
      "         [0.8876, 0.8081, 0.1627,  ..., 0.9346, 0.8919, 0.8500],\n",
      "         [0.1904, 0.8035, 0.7967,  ..., 0.0284, 0.9257, 0.1437],\n",
      "         ...,\n",
      "         [0.0681, 0.5483, 0.3325,  ..., 0.0230, 0.5271, 0.0218],\n",
      "         [0.0327, 0.0365, 0.7059,  ..., 0.0068, 0.9002, 0.2723],\n",
      "         [0.7763, 0.6677, 0.4112,  ..., 0.1131, 0.4803, 0.3788]],\n",
      "\n",
      "        [[0.8028, 0.4473, 0.2510,  ..., 0.4142, 0.3523, 0.9122],\n",
      "         [0.2820, 0.6596, 0.1280,  ..., 0.5229, 0.9805, 0.7835],\n",
      "         [0.3198, 0.4954, 0.4364,  ..., 0.3871, 0.2739, 0.9260],\n",
      "         ...,\n",
      "         [0.7014, 0.0530, 0.0046,  ..., 0.7224, 0.6271, 0.4949],\n",
      "         [0.6403, 0.7565, 0.9172,  ..., 0.6763, 0.6231, 0.6249],\n",
      "         [0.8127, 0.1359, 0.9042,  ..., 0.4618, 0.5075, 0.4233]],\n",
      "\n",
      "        [[0.5053, 0.6740, 0.8602,  ..., 0.6246, 0.6304, 0.9178],\n",
      "         [0.8350, 0.6646, 0.8395,  ..., 0.7426, 0.9119, 0.5501],\n",
      "         [0.7471, 0.1517, 0.6479,  ..., 0.1247, 0.5989, 0.6890],\n",
      "         ...,\n",
      "         [0.8673, 0.7595, 0.0323,  ..., 0.8126, 0.9667, 0.8061],\n",
      "         [0.1936, 0.0409, 0.3156,  ..., 0.6990, 0.1553, 0.9646],\n",
      "         [0.8022, 0.0563, 0.8337,  ..., 0.9390, 0.6487, 0.9218]]]), tensor([[[0.7948, 0.1624, 0.3237,  ..., 0.9684, 0.7733, 0.8907],\n",
      "         [0.1313, 0.9918, 0.1382,  ..., 0.2928, 0.7528, 0.1762],\n",
      "         [0.1144, 0.1148, 0.0284,  ..., 0.0112, 0.7524, 0.4614],\n",
      "         ...,\n",
      "         [0.2001, 0.6392, 0.7234,  ..., 0.5931, 0.0715, 0.0089],\n",
      "         [0.1761, 0.1509, 0.5318,  ..., 0.4618, 0.0633, 0.9954],\n",
      "         [0.5773, 0.9525, 0.3640,  ..., 0.9348, 0.0328, 0.0240]],\n",
      "\n",
      "        [[0.1727, 0.4290, 0.0926,  ..., 0.8633, 0.5585, 0.5317],\n",
      "         [0.3336, 0.5810, 0.3791,  ..., 0.5557, 0.3060, 0.0945],\n",
      "         [0.1166, 0.3851, 0.5448,  ..., 0.3261, 0.7491, 0.2684],\n",
      "         ...,\n",
      "         [0.8048, 0.8040, 0.1296,  ..., 0.4442, 0.0620, 0.0727],\n",
      "         [0.2039, 0.0134, 0.9391,  ..., 0.8935, 0.5497, 0.4897],\n",
      "         [0.5587, 0.1759, 0.5742,  ..., 0.0530, 0.0579, 0.6900]],\n",
      "\n",
      "        [[0.2637, 0.8812, 0.5880,  ..., 0.0449, 0.0816, 0.3582],\n",
      "         [0.9777, 0.8918, 0.4736,  ..., 0.3659, 0.2043, 0.4779],\n",
      "         [0.6825, 0.3044, 0.8392,  ..., 0.5898, 0.9804, 0.3137],\n",
      "         ...,\n",
      "         [0.0789, 0.7826, 0.2558,  ..., 0.3573, 0.5156, 0.1888],\n",
      "         [0.7888, 0.3289, 0.2157,  ..., 0.1854, 0.6657, 0.9778],\n",
      "         [0.3396, 0.9883, 0.2380,  ..., 0.7439, 0.5661, 0.2117]]])]\n",
      "torch.Size([4, 11, 4])\n",
      "torch.Size([4, 11])\n",
      "[{'boxes': tensor([[2.8381e-02, 4.1938e-04, 2.8778e-01, 5.4901e-01],\n",
      "        [7.5920e-01, 1.9359e-01, 5.8906e-01, 1.4630e-01],\n",
      "        [9.4812e-01, 7.9190e-01, 5.0732e-01, 5.7187e-01],\n",
      "        [4.7256e-01, 4.6459e-01, 2.7894e-01, 9.8543e-01],\n",
      "        [1.7774e-01, 4.1892e-01, 2.7328e-01, 2.1932e-02],\n",
      "        [4.2666e-01, 5.2052e-01, 8.4326e-01, 4.4134e-01],\n",
      "        [6.7033e-01, 4.2681e-01, 4.4797e-01, 7.1135e-01],\n",
      "        [3.8357e-01, 3.8691e-01, 6.2080e-01, 7.8849e-01],\n",
      "        [8.1409e-01, 2.5660e-01, 9.2266e-01, 7.5165e-01],\n",
      "        [3.3143e-01, 6.5278e-01, 6.8959e-01, 3.5836e-01],\n",
      "        [4.7423e-01, 5.8278e-01, 1.0813e-01, 5.9325e-01]]), 'labels': tensor([38,  1, 35, 10, 14, 74,  3,  6, 49, 21, 88])}, {'boxes': tensor([[0.0658, 0.2743, 0.2939, 0.4405],\n",
      "        [0.3421, 0.0357, 0.6014, 0.6328],\n",
      "        [0.9961, 0.3376, 0.2241, 0.8328],\n",
      "        [0.8118, 0.4284, 0.2513, 0.4039],\n",
      "        [0.9857, 0.8789, 0.0957, 0.2237],\n",
      "        [0.0596, 0.8037, 0.9992, 0.2379],\n",
      "        [0.7633, 0.4071, 0.3236, 0.2627],\n",
      "        [0.3676, 0.3619, 0.8140, 0.0932],\n",
      "        [0.6237, 0.0193, 0.5064, 0.0271],\n",
      "        [0.8735, 0.8294, 0.5318, 0.9877],\n",
      "        [0.7692, 0.5870, 0.4256, 0.9494]]), 'labels': tensor([40, 62, 58, 23, 34, 40,  7, 45, 34, 84, 70])}, {'boxes': tensor([[0.7001, 0.8589, 0.0647, 0.8065],\n",
      "        [0.8113, 0.0425, 0.2069, 0.9859],\n",
      "        [0.6001, 0.4150, 0.3286, 0.7455],\n",
      "        [0.8188, 0.5804, 0.2315, 0.8467],\n",
      "        [0.6308, 0.9819, 0.1336, 0.5147],\n",
      "        [0.4538, 0.4962, 0.3685, 0.3190],\n",
      "        [0.5597, 0.5886, 0.1108, 0.6835],\n",
      "        [0.7339, 0.9479, 0.4496, 0.4028],\n",
      "        [0.3213, 0.8348, 0.8032, 0.4505],\n",
      "        [0.2573, 0.3878, 0.7976, 0.8754],\n",
      "        [0.2099, 0.4408, 0.5862, 0.1540]]), 'labels': tensor([31, 45, 38, 11, 54, 26, 52, 48, 14, 65, 11])}, {'boxes': tensor([[0.9892, 0.7874, 0.8345, 0.8355],\n",
      "        [0.6542, 0.8523, 0.5862, 0.5489],\n",
      "        [0.5633, 0.1378, 0.2257, 0.3536],\n",
      "        [0.2004, 0.5405, 0.5386, 0.2449],\n",
      "        [0.5284, 0.6452, 0.3385, 0.8667],\n",
      "        [0.3189, 0.5862, 0.6281, 0.1205],\n",
      "        [0.7268, 0.9275, 0.4712, 0.4153],\n",
      "        [0.4874, 0.5828, 0.5840, 0.0821],\n",
      "        [0.2688, 0.4105, 0.4468, 0.5256],\n",
      "        [0.3600, 0.9372, 0.1109, 0.0996],\n",
      "        [0.6249, 0.5442, 0.9493, 0.9099]]), 'labels': tensor([58, 57, 53, 39, 56, 31, 45, 16, 39, 83, 64])}]\n",
      "{'loss_classifier': tensor(0.0745, grad_fn=<NllLossBackward>), 'loss_box_reg': tensor(0.0023, grad_fn=<DivBackward0>), 'loss_objectness': tensor(13.9195, grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_rpn_box_reg': tensor(nan, grad_fn=<DivBackward0>)}\n"
     ]
    }
   ],
   "source": [
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "# For training\n",
    "images, boxes = torch.rand(4, 3, 600, 1200), torch.rand(4, 11, 4)\n",
    "labels = torch.randint(1, 91, (4, 11))\n",
    "images = list(image for image in images)\n",
    "print(images)\n",
    "print(boxes.shape)\n",
    "print(labels.shape)\n",
    "targets = []\n",
    "for i in range(len(images)):\n",
    "    d = {}\n",
    "    d['boxes'] = boxes[i]\n",
    "    d['labels'] = labels[i]\n",
    "    targets.append(d)\n",
    "print(targets)\n",
    "output = model(images, targets)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compare_transforms' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-00cfebf32d32>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mtrain_loaded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mval_loaded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mcompare_transforms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_folder1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mCarDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'compare_transforms' is not defined"
     ]
    }
   ],
   "source": [
    "train_path = Path.cwd() / \"train\"\n",
    "val_path = Path.cwd() / \"val\"\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "transform1 = transforms.Compose([transforms.ToTensor()])\n",
    "train_folder = ImageFolder(train_path, transform=transform)\n",
    "train_folder1 = ImageFolder(train_path, transform=transform1)\n",
    "val_folder = ImageFolder(val_path, transform=transform)\n",
    "\n",
    "train_loaded = DataLoader(train_folder, batch_size=32, shuffle = True)\n",
    "val_loaded = DataLoader(val_folder, batch_size=32)\n",
    "compare_transforms([train_folder, train_folder1], 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x000001CF1C745508>\n"
     ]
    }
   ],
   "source": [
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "classes = 196\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features,classes)\n",
    "\n",
    "class CarDataset(Dataset):\n",
    "    def __init__(self, df, image_dir, labels, transforms= None):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.image_ids = self.df['image_id']\n",
    "        self.image_dir = image_dir\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        records = self.df[self.df[\"image_id\"]==image_id]\n",
    "        img = Image.open(self.image_dir/image_id)\n",
    "        img = transforms.ToTensor()(img)\n",
    "        boxes = records[[\"x0\",\"y0\", \"x1\", \"y1\"]].values\n",
    "        boxes = torch.tensor(boxes, dtype=torch.int64)\n",
    "        labels = torch.tensor(self.labels[idx])\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = torch.tensor([idx])\n",
    "        return img, target, image_id\n",
    "\n",
    "                            \n",
    "\n",
    "                                  \n",
    "    def __len__(self):\n",
    "        return self.image_ids.shape[0]\n",
    "\n",
    "dataset = CarDataset(df, Path.cwd() / \"train_all\", f1)\n",
    "datloaded = DataLoader(dataset, batch_size=32)        \n",
    "print(datloaded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 224, 224])\n",
      "{'boxes': tensor([[ 14,  43, 212, 140],\n",
      "        [  8,  28, 216, 146],\n",
      "        [ 29,  38, 210, 133],\n",
      "        [ 66,  41, 158, 116],\n",
      "        [ 21,  56, 206, 154],\n",
      "        [100, 112, 200, 162],\n",
      "        [ 30,  28, 189, 138],\n",
      "        [ 25,  27, 207, 143],\n",
      "        [  3,  22, 222, 134],\n",
      "        [  7,  38, 218, 128],\n",
      "        [ 17,  32, 210, 137],\n",
      "        [  2,  27, 223, 128],\n",
      "        [ 13,  16, 187, 137],\n",
      "        [  8,  68, 217, 151],\n",
      "        [ 11,  26, 206, 132],\n",
      "        [  9,  17, 213, 138],\n",
      "        [ 33,  44, 201, 129],\n",
      "        [  3,   8, 212, 164],\n",
      "        [ 36,  42, 203, 113],\n",
      "        [  3,  62, 215, 133],\n",
      "        [  5,  46, 207, 112],\n",
      "        [ 22,  57, 201, 120],\n",
      "        [  4,  12, 213, 104],\n",
      "        [ 11,  27, 212, 135],\n",
      "        [ 12,  32, 210, 127],\n",
      "        [ 10,  62, 213, 135],\n",
      "        [ 28,  38, 215, 119],\n",
      "        [ 16,  41, 211, 138],\n",
      "        [  3,  19, 218, 157],\n",
      "        [ 20,  27, 218, 130],\n",
      "        [ 12,  69,  97, 123],\n",
      "        [  5,  13, 211, 156]], device='cuda:0'), 'labels': tensor([ 14,   3,  91, 134, 106, 123,  89,  96, 167,  58,  49, 186, 135,  85,\n",
      "        193, 172,  14,  73, 192,  57,  79,  36, 120, 170, 194, 134, 184,  86,\n",
      "        180, 194, 154, 139], device='cuda:0')}\n",
      "{'boxes': tensor([[ 14,  43, 212, 140],\n",
      "        [  8,  28, 216, 146],\n",
      "        [ 29,  38, 210, 133],\n",
      "        [ 66,  41, 158, 116],\n",
      "        [ 21,  56, 206, 154],\n",
      "        [100, 112, 200, 162],\n",
      "        [ 30,  28, 189, 138],\n",
      "        [ 25,  27, 207, 143],\n",
      "        [  3,  22, 222, 134],\n",
      "        [  7,  38, 218, 128],\n",
      "        [ 17,  32, 210, 137],\n",
      "        [  2,  27, 223, 128],\n",
      "        [ 13,  16, 187, 137],\n",
      "        [  8,  68, 217, 151],\n",
      "        [ 11,  26, 206, 132],\n",
      "        [  9,  17, 213, 138],\n",
      "        [ 33,  44, 201, 129],\n",
      "        [  3,   8, 212, 164],\n",
      "        [ 36,  42, 203, 113],\n",
      "        [  3,  62, 215, 133],\n",
      "        [  5,  46, 207, 112],\n",
      "        [ 22,  57, 201, 120],\n",
      "        [  4,  12, 213, 104],\n",
      "        [ 11,  27, 212, 135],\n",
      "        [ 12,  32, 210, 127],\n",
      "        [ 10,  62, 213, 135],\n",
      "        [ 28,  38, 215, 119],\n",
      "        [ 16,  41, 211, 138],\n",
      "        [  3,  19, 218, 157],\n",
      "        [ 20,  27, 218, 130],\n",
      "        [ 12,  69,  97, 123],\n",
      "        [  5,  13, 211, 156]], device='cuda:0'), 'labels': tensor([ 14,   3,  91, 134, 106, 123,  89,  96, 167,  58,  49, 186, 135,  85,\n",
      "        193, 172,  14,  73, 192,  57,  79,  36, 120, 170, 194, 134, 184,  86,\n",
      "        180, 194, 154, 139], device='cuda:0')}\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-83-109854963701>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mout_put\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mb_y\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\dml\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\dml\\lib\\site-packages\\torchvision\\models\\detection\\generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, images, targets)\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"In training mode, targets should be passed\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0moriginal_image_sizes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\dml\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\dml\\lib\\site-packages\\torchvision\\models\\detection\\transform.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, images, targets)\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m             \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtargets\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m                 raise ValueError(\"images is expected to be a list of 3d tensors \"\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device);\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    for b_x, b_y, b_z in datloaded:\n",
    "        b_x,b_y = b_x.to(device),{'boxes':b_y[\"boxes\"].to(device),'labels': b_y[\"labels\"].to(device)}\n",
    "        print(b_x.shape)\n",
    "        b_y['boxes'] = np.squeeze(b_y['boxes'])\n",
    "        print(b_y)\n",
    "        print(b_y)\n",
    "        out_put=model(b_x,b_y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
