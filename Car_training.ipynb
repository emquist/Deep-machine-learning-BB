{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import shutil\n",
    "import random\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.datasets import ImageNet\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "#import torchvision.models as models\n",
    "#from torchvision.transforms import Compose\n",
    "#from torchvision.transforms import ToTensor\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resize(object):\n",
    "    def __init__(self, size, interpolation=Image.BILINEAR):\n",
    "        self.size = size\n",
    "        self.interpolation = interpolation\n",
    "        self.aspect_ratio = []\n",
    "\n",
    "    def __call__(self, img):\n",
    "        old_size = img.size  # old_size[0] is in (width, height) format\n",
    "\n",
    "        ratio = float(self.size)/max(old_size)\n",
    "        self.aspect_ratio.append(ratio)\n",
    "        new_size = tuple([int(x * ratio) for x in old_size])\n",
    "        \n",
    "        img=img.resize(new_size, resample=self.interpolation)\n",
    "        new_im = Image.new(\"RGB\", (self.size, self.size))\n",
    "        new_im.paste(img, ((self.size-new_size[0])//2,(self.size-new_size[1])//2))\n",
    "        return new_im\n",
    "#resize = Resize(224)\n",
    "#img = resize(Image.open('train_all_test/00010.jpg'))\n",
    "#img.save('train_all_test/00010.jpg') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2  = open(\"fname_string.txt\", \"r\").read().split(\"\\n\")\n",
    "\n",
    "resize = Resize(224)\n",
    "for fname in f2:\n",
    "    img = resize(Image.open('train_all/{}'.format(fname)))\n",
    "    img.save('train_all/{}'.format(fname)) \n",
    "AR=resize.aspect_ratio\n",
    "with open('aspectratio.txt', 'w') as f:\n",
    "    for item in AR:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all_path = Path.cwd() / \"train_all\"\n",
    "\n",
    "all_filenames = list(train_all_path.glob(\"*.jpg\"))\n",
    "all_filenames_length = len(all_filenames)\n",
    "#print(all_filenames)\n",
    "\n",
    "train_filenames = random.sample(all_filenames, int(0.8*all_filenames_length))\n",
    "#print(train_filenames)\n",
    "val_filenames=[]\n",
    "for file in all_filenames:\n",
    "    if file not in train_filenames:\n",
    "        val_filenames.append(file)\n",
    "        \n",
    "        \n",
    "for subdirectory in range(1,197):\n",
    "    subdirectory = Path(\"train/{}\".format(subdirectory))\n",
    "    subdirectory.mkdir(parents=True, exist_ok=True)\n",
    "for subdirectory in range(1,197):\n",
    "    subdirectory = Path(\"val/{}\".format(subdirectory))\n",
    "    subdirectory.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "f1  = open(\"class_string.txt\", \"r\").read().split(\"\\n\")\n",
    "     \n",
    "for index in range(all_filenames_length):\n",
    "    rand = np.random.rand()\n",
    "    if rand < 0.8:\n",
    "        shutil.copyfile(all_filenames[index], Path.cwd() / \"train/{}/{}\".format(f1[index],f2[index]))\n",
    "    else:\n",
    "        shutil.copyfile(all_filenames[index], Path.cwd() / \"val/{}/{}\".format(f1[index],f2[index]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f3 = open(\"bbox.txt\", \"r\").read().split(\"\\n\")\n",
    "f1  = open(\"class_string.txt\", \"r\").read().split(\"\\n\")\n",
    "f4  = open(\"aspectratio.txt\", \"r\").read().split(\"\\n\")\n",
    "AR=[]\n",
    "for line in f4:\n",
    "    try:\n",
    "        AR.append(float(line))\n",
    "    except:\n",
    "        pass\n",
    "totbbox = []\n",
    "for i,line in enumerate(f3):\n",
    "    bbox = line.split(\"\\t\") # x1,y1,x2,y2\n",
    "    bbox[0] = int(int(bbox[0])*AR[i])\n",
    "    bbox[1] = int(int(bbox[1])*AR[i])\n",
    "    bbox[2] = int(int(bbox[2])*AR[i])\n",
    "    bbox[3] = int(int(bbox[3])*AR[i])\n",
    "    totbbox.append(bbox)\n",
    "    f1[i] = int(f1[i])\n",
    "#totbbox = torch.tensor(totbbox,dtype=torch.float32)\n",
    "\n",
    "fin_list = []\n",
    "for i in range(len(f2)):\n",
    "    fin_list.append([f2[i],totbbox[i][0],totbbox[i][1],totbbox[i][2],totbbox[i][3]])\n",
    "print(fin_list[0][:])\n",
    "df = pd.DataFrame(fin_list, columns=['image_id', 'x0','y0','x1','y1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Handler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_transforms(transformations, index):\n",
    "    \"\"\"Visually compare transformations side by side.\n",
    "    Takes a list of ImageFolder datasets with different compositions of transformations.\n",
    "    It then display the `index`th image of the dataset for each transformed dataset in the list.\n",
    "    \n",
    "    Example usage:\n",
    "        compare_transforms([dataset_with_transform_1, dataset_with_transform_2], 0)\n",
    "    \n",
    "    Args:\n",
    "        transformations (list(ImageFolder)): list of ImageFolder instances with different transformations\n",
    "        index (int): Index of the sample in the ImageFolder you wish to compare.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Here we combine two neat functions from basic python to validate the input to the function:\n",
    "    # - `all` takes an iterable (something we can loop over, like a list) of booleans\n",
    "    #    and returns True if every element is True, otherwise it returns False.\n",
    "    # - `isinstance` checks whether a variable is an instance of a particular type (class)\n",
    "    if not all(isinstance(transf, ImageFolder) for transf in transformations):\n",
    "        raise TypeError(\"All elements in the `transformations` list need to be of type ImageFolder\")\n",
    "        \n",
    "    num_transformations = len(transformations)\n",
    "    fig, axes = plt.subplots(1, num_transformations)\n",
    "    \n",
    "    # This is just a hack to make sure that `axes` is a list of the same length as `transformations`.\n",
    "    # If we only have one element in the list, `plt.subplots` will not create a list of a single axis\n",
    "    # but rather just an axis without a list.\n",
    "    if num_transformations == 1:\n",
    "        axes = [axes]\n",
    "        \n",
    "    for counter, (axis, transf) in enumerate(zip(axes, transformations)):\n",
    "        axis.set_title(\"transf: {}\".format(counter))\n",
    "        image_tensor = transf[index][0]\n",
    "        display_image(axis, image_tensor)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def display_image(axis, image_tensor):\n",
    "    \"\"\"Display a tensor as image\n",
    "    \n",
    "    Example usage:\n",
    "        _, axis = plt.subplots()\n",
    "        some_random_index = 453\n",
    "        image_tensor, _ = train_dataset[some_random_index]\n",
    "        display_image(axis, image_tensor)\n",
    "    \n",
    "    Args:\n",
    "        axis (pyplot axis)\n",
    "        image_tensor (torch.Tensor): tensor with shape (num_channels=3, width, heigth)\n",
    "    \"\"\"\n",
    "    \n",
    "    # See hint above\n",
    "    if not isinstance(image_tensor, torch.Tensor):\n",
    "        raise TypeError(\"The `display_image` function expects a `torch.Tensor` \" +\n",
    "                        \"use the `ToTensor` transformation to convert the images to tensors.\")\n",
    "        \n",
    "    # The imshow commands expects a `numpy array` with shape (3, width, height)\n",
    "    # We rearrange the dimensions with `permute` and then convert it to `numpy`\n",
    "    image_data = image_tensor.permute(1, 2, 0).numpy()\n",
    "    height, width, _ = image_data.shape\n",
    "    axis.imshow(image_data)\n",
    "    axis.set_xlim(0, width)\n",
    "    # By convention when working with images, the origin is at the top left corner.\n",
    "    # Therefore, we switch the order of the y limits.\n",
    "    axis.set_ylim(height, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "# For training\n",
    "images, boxes = torch.rand(4, 3, 600, 1200), torch.rand(4, 11, 4)\n",
    "labels = torch.randint(1, 91, (4, 11))\n",
    "images = list(image for image in images)\n",
    "print(images)\n",
    "print(boxes.shape)\n",
    "print(labels.shape)\n",
    "targets = []\n",
    "for i in range(len(images)):\n",
    "    d = {}\n",
    "    d['boxes'] = boxes[i]\n",
    "    d['labels'] = labels[i]\n",
    "    targets.append(d)\n",
    "print(targets)\n",
    "output = model(images, targets)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = Path.cwd() / \"train\"\n",
    "val_path = Path.cwd() / \"val\"\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "transform1 = transforms.Compose([transforms.ToTensor()])\n",
    "train_folder = ImageFolder(train_path, transform=transform)\n",
    "train_folder1 = ImageFolder(train_path, transform=transform1)\n",
    "val_folder = ImageFolder(val_path, transform=transform)\n",
    "\n",
    "train_loaded = DataLoader(train_folder, batch_size=32, shuffle = True)\n",
    "val_loaded = DataLoader(val_folder, batch_size=32)\n",
    "compare_transforms([train_folder, train_folder1], 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "classes = 196\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features,classes)\n",
    "\n",
    "class CarDataset(Dataset):\n",
    "    def __init__(self, df, image_dir, labels, transforms= None):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.image_ids = self.df['image_id']\n",
    "        self.image_dir = image_dir\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        records = self.df[self.df[\"image_id\"]==image_id]\n",
    "        img = Image.open(self.image_dir/image_id)\n",
    "        img = transforms.ToTensor()(img)\n",
    "        boxes = records[[\"x0\",\"y0\", \"x1\", \"y1\"]].values\n",
    "        boxes = torch.tensor(boxes, dtype=torch.int64)\n",
    "        labels = torch.tensor(self.labels[idx])\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = torch.tensor([idx])\n",
    "        return img, target, image_id\n",
    "\n",
    "                            \n",
    "\n",
    "                                  \n",
    "    def __len__(self):\n",
    "        return self.image_ids.shape[0]\n",
    "\n",
    "dataset = CarDataset(df, Path.cwd() / \"train_all\", f1)\n",
    "datloaded = DataLoader(dataset, batch_size=32)        \n",
    "print(datloaded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device);\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    for b_x, b_y, b_z in datloaded:\n",
    "        b_x,b_y = b_x.to(device),{'boxes':b_y[\"boxes\"].to(device),'labels': b_y[\"labels\"].to(device)}\n",
    "        print(b_x.shape)\n",
    "        b_y['boxes'] = np.squeeze(b_y['boxes'])\n",
    "        print(b_y)\n",
    "        print(b_y)\n",
    "        out_put=model(b_x,b_y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
